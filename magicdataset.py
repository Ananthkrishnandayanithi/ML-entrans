# -*- coding: utf-8 -*-
"""magicdataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y4jh8mKqLydy7MMs3CGTdn9eFhPT_AlT
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from  imblearn.over_sampling import RandomOverSampler

column = ['fLength','fWidth','fSize','fConc','fConc1','fAsym','fM3Long','fM3Trans','fAlpha','fDist','class']
df = pd.read_csv('magic04.data',names=column)

df.head()

df['class'].unique()



df['class'] = (df['class'] == 'g').astype(int)

df.head()

for i in column[:-1]:
    plt.hist(df[df['class'] == 1][i], color='blue', label='gamma', alpha=0.7, density=True)
    plt.hist(df[df['class'] == 0][i], color='red', label='notgamma', alpha=0.7, density=True)
    plt.ylabel('Probability')
    plt.xlabel(i)
    plt.legend()
    plt.title(f'Histogram of {i}')
    plt.show()

train,valida,test = np.split(df.sample(frac=1),[int(0.6*len(df)),int(0.8*len(df))])

from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import RandomOverSampler
import numpy as np

def scale_dataset(df, oversam ple=False):
    # Separate features (x) and target (y)
    x = df[df.columns[:-1]].values  # All columns except the last one
    y = df[df.columns[-1]].values   # Only the last column

    # Initialize and apply StandardScaler to standardize features
    scaler = StandardScaler()
    x = scaler.fit_transform(x)

    # If oversampling is requested
    if oversample:
        ros = RandomOverSampler()
        x, y = ros.fit_resample(x, y)  # Resample to balance classes

    # Combine scaled features and target back into a single array
    data = np.hstack((x, np.reshape(y, (-1, 1))))

    return x, y, data

x_train, y_train, train = scale_dataset(train, oversample=True)
x_valid, y_valid, valid = scale_dataset(valida, oversample=False)
x_test, y_test, test = scale_dataset(test, oversample=False)

len(y_train)



from imblearn.over_sampling import RandomOverSampler

from sklearn.neighbors import KNeighborsClassifier



knnmodel = KNeighborsClassifier(n_neighbors=3)
knnmodel.fit(x_train,y_train)

y_pred = knnmodel.predict(x_test)

y_pred

y_test

from sklearn.metrics import classification_report
print(classification_report(y_pred,y_test))

from sklearn.naive_bayes import GaussianNB

gaussmodel = GaussianNB()
gaussmodel.fit(x_train,y_train)

y_pred = gaussmodel.predict(x_test)

y_pred

y_test

print(classification_report(y_pred,y_test))

from sklearn.linear_model import LogisticRegression
logicmodel = LogisticRegression()

logicmodel.fit(x_train,y_train)

y_predlo = logicmodel.predict(x_test)

print(classification_report(y_predlo,y_test))

from sklearn.svm import SVC

svcmodel = SVC()

svcmodel.fit(x_train,y_train)

y_predsvc = svcmodel.predict(x_test)

print(y_predsvc)

print(y_test)

print(classification_report(y_predsvc,y_test))



import tensorflow as tf

nm_model = tf.keras.Sequential(
    [tf.keras.layers.Dense(32,activation= 'relu',input_shape=(10,)),
     tf.keras.layers.Dense(32,activation= 'relu'),
     tf.keras.layers.Dense(1,activation= 'sigmoid')
])
nm_model.compile(tf.keras.optimizers.Adam(0.001),loss = 'binary_crossentropy', metrics= ['accuracy'])

nm_model.fit(x_train,y_train,epochs=100,batch_size=32,validation_split=0.2)

y_pred = nm_model.predict(x_test)