# -*- coding: utf-8 -*-
"""telecom-customer-churn-prediction-with-ml.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OcGSxabwv_MUApLbPybqmra_SZP1pHQr

# Telecom Customer Churn Prediction

## Import Libraries
"""

import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
import seaborn as sns #
import plotly.express as px
from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot
import cufflinks as cf

import warnings

warnings.filterwarnings('ignore')
cf.go_offline()
init_notebook_mode(connected=True)

!pip install jupyterthemes

from jupyterthemes import jtplot
jtplot.style(theme= 'monokai', context= 'notebook', ticks= True, grid= False)

"""## Load Data"""

# Read a preprocessed version of the Dataset file
telecom_df = pd.read_csv("https://raw.githubusercontent.com/LasalJayawardena/ML-Projects/main/Telecom_Churn_Prediction/telecom_churn.csv")

telecom_df.head()

telecom_df.tail()

# Check the shape of the dataframe
telecom_df.shape

# Display the feature columns
telecom_df.columns

# Obtain the summary of the dataframe
telecom_df.info()

telecom_df.describe()

"""## Data Vizualisation

### Plot the Histograms for the Dataset
"""

telecom_df.hist(figsize = (30, 30))
plt.show()

"""### Pie Chart to get the information about the percentage of Telecom Customers churning using Plotly histogram"""

import plotly.graph_objects as go
fig = go.Figure(data = [go.Pie(labels = ["Exited (1)", "Retained (0)"], values = telecom_df["class"].value_counts())])
fig.show()

"""### Histogram graph for the international plan service used by the Telecom customers with respect to churned/Retained"""

fig = px.histogram(telecom_df, x = "international_plan",
                  color = "class",
                  title = "International Plan service opted by the Telecom Customers")
fig.show()

"""### Correlation Matrix"""

corr_matrix = telecom_df.corr()
plt.figure(figsize = (15, 15))
cm = sns.heatmap(corr_matrix,
               linewidths = 1,
               annot = True,
               fmt = ".2f")
plt.title("Correlation Matrix of Telecom Customers", fontsize = 20)
plt.show()

# It is clearly shown that "voice_mail_plan" and "number_vmail_messages" are highly correlated.
# It is clearly shown that "total day charge" and "total daily minutes" are highly correlated.

"""### Churn by day charges"""

ax = sns.kdeplot(telecom_df.total_day_charge[(telecom_df["class"] == 0)],
               color = "seagreen", shade = True)
ax = sns.kdeplot(telecom_df.total_day_charge[(telecom_df["class"] == 1)],
               color = "yellow", shade = True)

ax.legend(["Retain", "Churn"], loc = "upper right")
ax.set_ylabel("Density")
ax.set_xlabel("Day Charges")
ax.set_title("Distribution of day charges by churn");

"""### Churn by evening charges"""

ax = sns.kdeplot(telecom_df.total_eve_charge[(telecom_df["class"] == 0)],
               color = "yellow", shade = True)
ax = sns.kdeplot(telecom_df.total_eve_charge[(telecom_df["class"] == 1)],
               color = "white", shade = True)

ax.legend(["Retain", "Churn"], loc = "upper right")
ax.set_ylabel("Density")
ax.set_xlabel("Evening Charges")
ax.set_title("Distribution of evening charges by churn");

"""## Data Preprocessing

* Unnecessary features would decrease the training speed, the model interpretability and the generalization performance on the test data.
* Therefore, finding and selecting the most useful features in the dataset is crucial.
* Assigning input features to X and output (Churn) to y
"""

X = telecom_df.drop(["class", "area_code", "phone_number"], axis = "columns") # area_code and phone_number features are irrelevant to proceed further to train the model
y = telecom_df["class"]

X.shape

y.shape

"""## Train Test Split"""

# Perform train/test split
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 150)

y_train.shape, y_test.shape

"""## Identify Feature Importance with a Random Forest Classifier"""

from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier()
rf.fit(X_train, y_train.values.ravel())

# Plot the feature importance

feat_scores= pd.DataFrame({"Fraction of variables affected" : rf.feature_importances_},index = X.columns)
feat_scores= feat_scores.sort_values(by = "Fraction of variables affected")
feat_scores.plot(kind = "barh", figsize = (10, 5))
sns.despine()

"""## Train and Evaluate a Logistic Regressor"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix

model_LR = LogisticRegression(max_iter=100000)

model_LR.fit(X_train, y_train)

y_predict = model_LR.predict(X_test)

# precision is the ratio of TP/(TP+FP)
# recall is the ratio of TP/(TP+FN)
# F-beta score can be interpreted as a weighted harmonic mean of the precision and recall
# where an F-beta score reaches its best value at 1 and worst score at 0.
print(classification_report(y_test, y_predict))

cm = confusion_matrix(y_test, y_predict)
sns.heatmap(cm, annot = True);

"""## Train and Evaluate a Support Vector Machine"""

from sklearn.calibration import CalibratedClassifierCV # For probability score output
from sklearn.svm import LinearSVC

model_svc = LinearSVC(max_iter=100000)
model_svm = CalibratedClassifierCV(model_svc)
model_svm.fit(X_train, y_train)

y_predict = model_svm.predict(X_test)

print(classification_report(y_test, y_predict))

cm = confusion_matrix(y_test, y_predict)
sns.heatmap(cm, annot = True);

"""## Train and Evaluate a Random Forest Classifier"""

from sklearn.ensemble import RandomForestClassifier

model_rf = RandomForestClassifier()
model_rf.fit(X_train, y_train)

y_predict = model_rf.predict(X_test)

print(classification_report(y_test, y_predict))

cm = confusion_matrix(y_test, y_predict)
sns.heatmap(cm, annot = True);

"""## Train and Evaluate A K-Nearest Neighbor (KNN) Classifier"""

from sklearn.neighbors import KNeighborsClassifier

model_knn = KNeighborsClassifier()
model_knn.fit(X_train, y_train)

y_predict = model_knn.predict(X_test)



print(classification_report(y_test, y_predict))

cm = confusion_matrix(y_test, y_predict)
sns.heatmap(cm, annot = True);

"""## Train and Evaluate a Naive Bayes Classifier"""

from sklearn.naive_bayes import GaussianNB

model_gnb = GaussianNB()
model_gnb.fit(X_train, y_train)

y_predict = model_gnb.predict(X_test)

print(classification_report(y_test, y_predict))

cm = confusion_matrix(y_test, y_predict)
sns.heatmap(cm, annot = True);

"""## Compare Each Model with the ROC Curve"""

from sklearn.metrics import roc_curve

# ROC curve

fpr1, tpr1, thresh1 = roc_curve(y_test, model_LR.predict_proba(X_test)[:, 1], pos_label = 1)
fpr2, tpr2, thresh2 = roc_curve(y_test, model_svm.predict_proba(X_test)[:, 1], pos_label = 1)
fpr3, tpr3, thresh3 = roc_curve(y_test, model_rf.predict_proba(X_test)[:, 1], pos_label = 1)
fpr4, tpr4, thresh4 = roc_curve(y_test, model_knn.predict_proba(X_test)[:, 1], pos_label = 1)
fpr5, tpr5, thresh5 = roc_curve(y_test, model_gnb.predict_proba(X_test)[:, 1], pos_label = 1)

# AUC score

from sklearn.metrics import roc_auc_score

auc_score1 = roc_auc_score(y_test, model_LR.predict_proba(X_test)[:, 1])
auc_score2 = roc_auc_score(y_test, model_svm.predict_proba(X_test)[:, 1])
auc_score3 = roc_auc_score(y_test, model_rf.predict_proba(X_test)[:, 1])
auc_score4 = roc_auc_score(y_test, model_knn.predict_proba(X_test)[:, 1])
auc_score5 = roc_auc_score(y_test, model_gnb.predict_proba(X_test)[:, 1])

print("Logistic Regression: ", auc_score1) # Logistic Regression
print("Support Vector Machine: ", auc_score2) # Support Vector Machine
print("Random Forest: ", auc_score3) # Random Forest
print("K-Nearest Neighbors: ", auc_score4) # K-Nearest Neighbors
print("Naive Bayes: ", auc_score5) # Naive Bayes

plt.plot(fpr1, tpr1, linestyle = "--", color = "orange", label = "Logistic Regression")
plt.plot(fpr2, tpr2, linestyle = "--", color = "red", label = "SVM")
plt.plot(fpr3, tpr3, linestyle = "--", color = "green", label = "Random Forest")
plt.plot(fpr4, tpr4, linestyle = "--", color = "yellow", label = "KNN")
plt.plot(fpr5, tpr5, linestyle = "--", color = "white", label = "Naive bayes")

plt.title('Receiver Operator Characteristics (ROC)')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive rate')

plt.legend(loc = 'best')
plt.savefig('ROC', dpi = 300)
plt.show()

"""## Conclusion:

**The graph represents that Random Forest algorithm produced the best AUC. Therefore, it is clear that Random Forest model did a better job of classifying the churned/retained telecom customers.**
"""

